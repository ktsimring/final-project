{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "import pyedflib\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #Hide messy TensorFlow warnings\n",
    "warnings.filterwarnings(\"ignore\") #Hide messy Numpy warnings\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return Series(diff)\n",
    "\n",
    "def load_data(filename):\n",
    "    g = pyedflib.EdfReader(filename)\n",
    "    n = g.signals_in_file\n",
    "    signal_labels = g.getSignalLabels()\n",
    "    sig = np.zeros((n, g.getNSamples()[0]))\n",
    "    for i in np.arange(n):\n",
    "        len_sig = len(g.readSignal(i))\n",
    "        sig[i, :len_sig] = g.readSignal(i)\n",
    "    return sig\n",
    "def make_data(sig, seq_len, normalise_window):\n",
    "    \n",
    "\n",
    "    data = sig[0][:500]\n",
    "    if not normalise_window:\n",
    "        data = difference(data,1)\n",
    "    sequence_length = seq_len + 1\n",
    "    result = np.zeros((len(data) - sequence_length, sequence_length))\n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result[index] = data[index: index + sequence_length]\n",
    "    \n",
    "    if normalise_window:\n",
    "        result = normalise_windows(result)\n",
    "    \n",
    "    row = round(0.9 * result.shape[0])\n",
    "    train = result[:int(row), :]\n",
    "    #np.random.shuffle(train)\n",
    "    x_train = train[:, :-1]\n",
    "    y_train = train[:, -1]\n",
    "    x_test = result[int(row):, :-1]\n",
    "    y_test = result[int(row):, -1]\n",
    "\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))  \n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_windows(window_data):\n",
    "    normalised_data = []\n",
    "    for window in window_data:\n",
    "        normalised_window = [((float(p) / float(window[0])) - 1) for p in window]\n",
    "        normalised_data.append(normalised_window)\n",
    "    return normalised_data\n",
    "        \n",
    "def build_model(layers):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        input_shape=(layers[1], layers[0]),\n",
    "        output_dim=layers[1],\n",
    "        return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "        layers[2],\n",
    "        return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(\n",
    "        output_dim=layers[3]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    start = time.time()\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "    print(\"> Compilation Time : \", time.time() - start)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_point_by_point(model, data):\n",
    "    #Predict each timestep given the last sequence of true data, in effect only predicting 1 step ahead each time\n",
    "    predicted = model.predict(data)\n",
    "    predicted = np.reshape(predicted, (predicted.size,))\n",
    "    return predicted\n",
    "\n",
    "def predict_sequence_full(model, data, window_size):\n",
    "    #Shift the window by 1 new prediction each time, re-run predictions on new window\n",
    "    curr_frame = data[0]\n",
    "    predicted = []\n",
    "    for i in range(len(data)):\n",
    "        predicted.append(model.predict(curr_frame[newaxis,:,:])[0,0])\n",
    "        curr_frame = curr_frame[1:]\n",
    "        curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n",
    "    return predicted\n",
    "\n",
    "def predict_sequences_multiple(model, data, window_size, prediction_len):\n",
    "    #Predict sequence of 50 steps before shifting prediction run forward by 50 steps\n",
    "    prediction_seqs = []\n",
    "    for i in range(int(len(data)/prediction_len)):\n",
    "        curr_frame = data[i*prediction_len]\n",
    "        predicted = []\n",
    "        for j in range(prediction_len):\n",
    "            predicted.append(model.predict(curr_frame[newaxis,:,:])[0,0])\n",
    "            curr_frame = curr_frame[1:]\n",
    "            curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n",
    "        prediction_seqs.append(predicted)\n",
    "    return prediction_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(predicted_data, true_data, make_line):\n",
    "    fig = plt.figure(facecolor='white')\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(true_data, label='True Data')\n",
    "    plt.plot(predicted_data, label='Prediction')\n",
    "    plt.legend()\n",
    "    if make_line:\n",
    "        plt.axvline(x=y_train.shape[0] + 1, linestyle='--', color = 'b', label = 'future predicted points')\n",
    "    plt.show()\n",
    "\n",
    "def plot_results_multiple(predicted_data, true_data, prediction_len):\n",
    "    fig = plt.figure(facecolor='white')\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(true_data, label='True Data')\n",
    "    #Pad the list of predictions to shift it in the graph to it's correct start\n",
    "    for i, data in enumerate(predicted_data):\n",
    "        padding = [None for p in range(i * prediction_len)]\n",
    "        plt.plot(padding + data, label='Prediction')\n",
    "        plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_start_time = time.time()\n",
    "seq_len = 50\n",
    "\n",
    "print('> Loading data... ')\n",
    "sig = load_data('sample-data/SC4001E0-PSG.edf')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = make_data(sig, seq_len, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Data Loaded. Compiling...\n",
      "('> Compilation Time : ', 0.010864973068237305)\n",
      "Train on 382 samples, validate on 21 samples\n",
      "Epoch 1/450\n",
      "382/382 [==============================] - 1s 2ms/step - loss: 127.8604 - val_loss: 105.7377\n",
      "Epoch 2/450\n",
      "382/382 [==============================] - 0s 619us/step - loss: 125.4630 - val_loss: 99.0716\n",
      "Epoch 3/450\n",
      "382/382 [==============================] - 0s 657us/step - loss: 122.6352 - val_loss: 90.1862\n",
      "Epoch 4/450\n",
      "382/382 [==============================] - 0s 626us/step - loss: 120.0043 - val_loss: 83.9425\n",
      "Epoch 5/450\n",
      "382/382 [==============================] - 0s 655us/step - loss: 116.9900 - val_loss: 74.4598\n",
      "Epoch 6/450\n",
      "382/382 [==============================] - 0s 691us/step - loss: 114.4737 - val_loss: 65.0750\n",
      "Epoch 7/450\n",
      "382/382 [==============================] - 0s 644us/step - loss: 112.5326 - val_loss: 64.3515\n",
      "Epoch 8/450\n",
      "382/382 [==============================] - 0s 622us/step - loss: 112.6072 - val_loss: 55.1635\n",
      "Epoch 9/450\n",
      "382/382 [==============================] - 0s 628us/step - loss: 109.4906 - val_loss: 60.7293\n",
      "Epoch 10/450\n",
      "382/382 [==============================] - 0s 667us/step - loss: 109.0733 - val_loss: 59.3989\n",
      "Epoch 11/450\n",
      "382/382 [==============================] - 0s 620us/step - loss: 106.8248 - val_loss: 48.5024\n",
      "Epoch 12/450\n",
      "382/382 [==============================] - 0s 638us/step - loss: 109.1794 - val_loss: 48.9957\n",
      "Epoch 13/450\n",
      "382/382 [==============================] - 0s 654us/step - loss: 105.5244 - val_loss: 49.7174\n",
      "Epoch 14/450\n",
      "382/382 [==============================] - 0s 695us/step - loss: 103.8750 - val_loss: 46.8237\n",
      "Epoch 15/450\n",
      "382/382 [==============================] - 0s 661us/step - loss: 102.7736 - val_loss: 51.7604\n",
      "Epoch 16/450\n",
      "382/382 [==============================] - 0s 644us/step - loss: 102.5190 - val_loss: 41.9159\n",
      "Epoch 17/450\n",
      "382/382 [==============================] - 0s 658us/step - loss: 102.3267 - val_loss: 45.5191\n",
      "Epoch 18/450\n",
      "382/382 [==============================] - 0s 624us/step - loss: 102.4381 - val_loss: 52.3581\n",
      "Epoch 19/450\n",
      "382/382 [==============================] - 0s 619us/step - loss: 101.8690 - val_loss: 43.7543\n",
      "Epoch 20/450\n",
      "382/382 [==============================] - 0s 607us/step - loss: 99.4045 - val_loss: 42.8725\n",
      "Epoch 21/450\n",
      "382/382 [==============================] - 0s 637us/step - loss: 98.9735 - val_loss: 42.7606\n",
      "Epoch 22/450\n",
      "382/382 [==============================] - 0s 645us/step - loss: 99.5613 - val_loss: 41.0600\n",
      "Epoch 23/450\n",
      "382/382 [==============================] - 0s 637us/step - loss: 98.3667 - val_loss: 41.9745\n",
      "Epoch 24/450\n",
      "382/382 [==============================] - 0s 661us/step - loss: 96.9434 - val_loss: 48.6419\n",
      "Epoch 25/450\n",
      "382/382 [==============================] - 0s 686us/step - loss: 101.4844 - val_loss: 41.4090\n",
      "Epoch 26/450\n",
      "382/382 [==============================] - 0s 623us/step - loss: 97.8997 - val_loss: 42.6099\n",
      "Epoch 27/450\n",
      "382/382 [==============================] - 0s 662us/step - loss: 97.7297 - val_loss: 40.0265\n",
      "Epoch 28/450\n",
      "382/382 [==============================] - 0s 646us/step - loss: 97.3300 - val_loss: 42.0598\n",
      "Epoch 29/450\n",
      "382/382 [==============================] - 0s 598us/step - loss: 97.2088 - val_loss: 39.0070\n",
      "Epoch 30/450\n",
      "382/382 [==============================] - 0s 649us/step - loss: 96.9408 - val_loss: 40.0753\n",
      "Epoch 31/450\n",
      "382/382 [==============================] - 0s 628us/step - loss: 95.5251 - val_loss: 48.5918\n",
      "Epoch 32/450\n",
      "382/382 [==============================] - 0s 652us/step - loss: 94.4958 - val_loss: 41.6095\n",
      "Epoch 33/450\n",
      "382/382 [==============================] - 0s 660us/step - loss: 95.1972 - val_loss: 58.8910\n",
      "Epoch 34/450\n",
      "382/382 [==============================] - 0s 651us/step - loss: 96.9922 - val_loss: 43.4202\n",
      "Epoch 35/450\n",
      "382/382 [==============================] - 0s 644us/step - loss: 93.7911 - val_loss: 41.1742\n",
      "Epoch 36/450\n",
      "382/382 [==============================] - 0s 629us/step - loss: 94.5914 - val_loss: 44.1307\n",
      "Epoch 37/450\n",
      "382/382 [==============================] - 0s 619us/step - loss: 95.2233 - val_loss: 43.5140\n",
      "Epoch 38/450\n",
      "382/382 [==============================] - 0s 641us/step - loss: 93.5748 - val_loss: 46.2764\n",
      "Epoch 39/450\n",
      "382/382 [==============================] - 0s 624us/step - loss: 91.4229 - val_loss: 47.8665\n",
      "Epoch 40/450\n",
      "382/382 [==============================] - 0s 616us/step - loss: 92.1834 - val_loss: 54.7474\n",
      "Epoch 41/450\n",
      "382/382 [==============================] - 0s 634us/step - loss: 91.3108 - val_loss: 44.0256\n",
      "Epoch 42/450\n",
      "382/382 [==============================] - 0s 613us/step - loss: 93.3502 - val_loss: 56.6180\n",
      "Epoch 43/450\n",
      "382/382 [==============================] - 0s 658us/step - loss: 95.8974 - val_loss: 52.1995\n",
      "Epoch 44/450\n",
      "382/382 [==============================] - 0s 637us/step - loss: 93.3155 - val_loss: 47.9957\n",
      "Epoch 45/450\n",
      "382/382 [==============================] - 0s 631us/step - loss: 90.4136 - val_loss: 42.7562\n",
      "Epoch 46/450\n",
      "382/382 [==============================] - 0s 643us/step - loss: 91.0681 - val_loss: 49.8373\n",
      "Epoch 47/450\n",
      "382/382 [==============================] - 0s 617us/step - loss: 90.2406 - val_loss: 46.4138\n",
      "Epoch 48/450\n",
      "382/382 [==============================] - 0s 626us/step - loss: 89.7698 - val_loss: 54.9575\n",
      "Epoch 49/450\n",
      "382/382 [==============================] - 0s 641us/step - loss: 90.2519 - val_loss: 58.8693\n",
      "Epoch 50/450\n",
      "382/382 [==============================] - 0s 669us/step - loss: 90.5154 - val_loss: 52.2784\n",
      "Epoch 51/450\n",
      "382/382 [==============================] - 0s 669us/step - loss: 89.3968 - val_loss: 55.0776\n",
      "Epoch 52/450\n",
      "382/382 [==============================] - 0s 647us/step - loss: 88.1717 - val_loss: 43.9427\n",
      "Epoch 53/450\n",
      "382/382 [==============================] - 0s 641us/step - loss: 92.9360 - val_loss: 44.9012\n",
      "Epoch 54/450\n",
      "382/382 [==============================] - 0s 671us/step - loss: 89.9680 - val_loss: 56.5604\n",
      "Epoch 55/450\n",
      "382/382 [==============================] - 0s 647us/step - loss: 89.7566 - val_loss: 75.6341\n",
      "Epoch 56/450\n",
      "382/382 [==============================] - 0s 664us/step - loss: 90.9108 - val_loss: 58.3223\n",
      "Epoch 57/450\n",
      "382/382 [==============================] - 0s 648us/step - loss: 90.9902 - val_loss: 58.4965\n",
      "Epoch 58/450\n",
      "382/382 [==============================] - 0s 627us/step - loss: 87.8109 - val_loss: 48.9202\n",
      "Epoch 59/450\n",
      "382/382 [==============================] - 0s 679us/step - loss: 87.5550 - val_loss: 58.6608\n",
      "Epoch 60/450\n",
      "382/382 [==============================] - 0s 576us/step - loss: 92.1954 - val_loss: 58.4124\n",
      "Epoch 61/450\n",
      "382/382 [==============================] - 0s 601us/step - loss: 88.0858 - val_loss: 58.5766\n",
      "Epoch 62/450\n",
      "382/382 [==============================] - 0s 643us/step - loss: 87.5870 - val_loss: 57.3567\n",
      "Epoch 63/450\n",
      "382/382 [==============================] - 0s 627us/step - loss: 87.2247 - val_loss: 55.4975\n",
      "Epoch 64/450\n",
      "382/382 [==============================] - 0s 627us/step - loss: 86.6734 - val_loss: 63.5029\n",
      "Epoch 65/450\n",
      "382/382 [==============================] - 0s 635us/step - loss: 86.0115 - val_loss: 51.7085\n",
      "Epoch 66/450\n",
      "382/382 [==============================] - 0s 643us/step - loss: 87.1408 - val_loss: 58.1548\n",
      "Epoch 67/450\n",
      "382/382 [==============================] - 0s 661us/step - loss: 87.7739 - val_loss: 66.6009\n",
      "Epoch 68/450\n",
      "382/382 [==============================] - 0s 604us/step - loss: 86.0946 - val_loss: 57.6628\n",
      "Epoch 69/450\n",
      "382/382 [==============================] - 0s 594us/step - loss: 87.0651 - val_loss: 54.4265\n",
      "Epoch 70/450\n",
      "382/382 [==============================] - 0s 641us/step - loss: 87.3160 - val_loss: 53.5894\n",
      "Epoch 71/450\n",
      "382/382 [==============================] - 0s 671us/step - loss: 86.8785 - val_loss: 67.8917\n",
      "Epoch 72/450\n",
      "382/382 [==============================] - 0s 607us/step - loss: 86.5239 - val_loss: 47.0510\n",
      "Epoch 73/450\n",
      "382/382 [==============================] - 0s 668us/step - loss: 87.1903 - val_loss: 47.7089\n",
      "Epoch 74/450\n",
      "382/382 [==============================] - 0s 632us/step - loss: 86.6165 - val_loss: 57.8797\n",
      "Epoch 75/450\n",
      "382/382 [==============================] - 0s 636us/step - loss: 82.6234 - val_loss: 52.9579\n",
      "Epoch 76/450\n",
      "382/382 [==============================] - 0s 640us/step - loss: 85.6696 - val_loss: 47.5817\n",
      "Epoch 77/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 677us/step - loss: 84.0852 - val_loss: 57.7472\n",
      "Epoch 78/450\n",
      "382/382 [==============================] - 0s 606us/step - loss: 84.1541 - val_loss: 74.9968\n",
      "Epoch 79/450\n",
      "382/382 [==============================] - 0s 643us/step - loss: 84.4345 - val_loss: 61.1960\n",
      "Epoch 80/450\n",
      "382/382 [==============================] - 0s 643us/step - loss: 82.2215 - val_loss: 60.5988\n",
      "Epoch 81/450\n",
      "382/382 [==============================] - 0s 651us/step - loss: 82.8188 - val_loss: 44.2516\n",
      "Epoch 82/450\n",
      "382/382 [==============================] - 0s 615us/step - loss: 85.1135 - val_loss: 73.2447\n",
      "Epoch 83/450\n",
      "382/382 [==============================] - 0s 608us/step - loss: 83.9513 - val_loss: 49.2949\n",
      "Epoch 84/450\n",
      "382/382 [==============================] - 0s 660us/step - loss: 83.0437 - val_loss: 54.2191\n",
      "Epoch 85/450\n",
      "382/382 [==============================] - 0s 651us/step - loss: 81.7972 - val_loss: 57.8785\n",
      "Epoch 86/450\n",
      "382/382 [==============================] - 0s 693us/step - loss: 82.2694 - val_loss: 59.4810\n",
      "Epoch 87/450\n",
      "382/382 [==============================] - 0s 608us/step - loss: 82.9557 - val_loss: 75.4682\n",
      "Epoch 88/450\n",
      "382/382 [==============================] - 0s 596us/step - loss: 85.0289 - val_loss: 56.4194\n",
      "Epoch 89/450\n",
      "382/382 [==============================] - 0s 648us/step - loss: 81.4655 - val_loss: 58.9526\n",
      "Epoch 90/450\n",
      "382/382 [==============================] - 0s 646us/step - loss: 81.4646 - val_loss: 53.7226\n",
      "Epoch 91/450\n",
      "382/382 [==============================] - 0s 656us/step - loss: 83.4595 - val_loss: 49.4179\n",
      "Epoch 92/450\n",
      "382/382 [==============================] - 0s 638us/step - loss: 81.9342 - val_loss: 68.9035\n",
      "Epoch 93/450\n",
      "382/382 [==============================] - 0s 634us/step - loss: 79.6141 - val_loss: 51.3554\n",
      "Epoch 94/450\n",
      "382/382 [==============================] - 0s 592us/step - loss: 78.5374 - val_loss: 61.7853\n",
      "Epoch 95/450\n",
      "382/382 [==============================] - 0s 628us/step - loss: 79.2588 - val_loss: 49.2396\n",
      "Epoch 96/450\n",
      "382/382 [==============================] - 0s 660us/step - loss: 84.0297 - val_loss: 65.0063\n",
      "Epoch 97/450\n",
      "382/382 [==============================] - 0s 616us/step - loss: 81.8605 - val_loss: 50.4479\n",
      "Epoch 98/450\n",
      "382/382 [==============================] - 0s 599us/step - loss: 80.5420 - val_loss: 50.3370\n",
      "Epoch 99/450\n",
      "382/382 [==============================] - 0s 649us/step - loss: 77.9566 - val_loss: 62.8066\n",
      "Epoch 100/450\n",
      "382/382 [==============================] - 0s 648us/step - loss: 76.5960 - val_loss: 59.4023\n",
      "Epoch 101/450\n",
      "382/382 [==============================] - 0s 683us/step - loss: 76.5408 - val_loss: 58.4092\n",
      "Epoch 102/450\n",
      "382/382 [==============================] - 0s 656us/step - loss: 76.8124 - val_loss: 62.3940\n",
      "Epoch 103/450\n",
      "382/382 [==============================] - 0s 655us/step - loss: 81.0500 - val_loss: 60.7140\n",
      "Epoch 104/450\n",
      "382/382 [==============================] - 0s 670us/step - loss: 76.9407 - val_loss: 46.0615\n",
      "Epoch 105/450\n",
      "382/382 [==============================] - 0s 666us/step - loss: 76.2045 - val_loss: 72.9692\n",
      "Epoch 106/450\n",
      "382/382 [==============================] - 0s 649us/step - loss: 79.0000 - val_loss: 49.1341\n",
      "Epoch 107/450\n",
      "382/382 [==============================] - 0s 623us/step - loss: 77.4363 - val_loss: 41.8825\n",
      "Epoch 108/450\n",
      "382/382 [==============================] - 0s 647us/step - loss: 77.8362 - val_loss: 58.3399\n",
      "Epoch 109/450\n",
      "382/382 [==============================] - 0s 631us/step - loss: 75.6919 - val_loss: 73.5802\n",
      "Epoch 110/450\n",
      "382/382 [==============================] - 0s 662us/step - loss: 76.9877 - val_loss: 56.9247\n",
      "Epoch 111/450\n",
      "382/382 [==============================] - 0s 628us/step - loss: 72.9743 - val_loss: 49.6005\n",
      "Epoch 112/450\n",
      "382/382 [==============================] - 0s 639us/step - loss: 75.7032 - val_loss: 47.4761\n",
      "Epoch 113/450\n",
      "382/382 [==============================] - 0s 632us/step - loss: 77.3622 - val_loss: 42.8738\n",
      "Epoch 114/450\n",
      "382/382 [==============================] - 0s 680us/step - loss: 71.1899 - val_loss: 62.4355\n",
      "Epoch 115/450\n",
      "382/382 [==============================] - 0s 638us/step - loss: 73.8224 - val_loss: 52.1601\n",
      "Epoch 116/450\n",
      "382/382 [==============================] - 0s 646us/step - loss: 74.7347 - val_loss: 52.9580\n",
      "Epoch 117/450\n",
      "382/382 [==============================] - 0s 608us/step - loss: 74.0069 - val_loss: 51.6406\n",
      "Epoch 118/450\n",
      "382/382 [==============================] - 0s 673us/step - loss: 71.8256 - val_loss: 69.6023\n",
      "Epoch 119/450\n",
      "382/382 [==============================] - 0s 680us/step - loss: 73.5480 - val_loss: 61.0536\n",
      "Epoch 120/450\n",
      "382/382 [==============================] - 0s 632us/step - loss: 71.6530 - val_loss: 53.8537\n",
      "Epoch 121/450\n",
      "382/382 [==============================] - 0s 690us/step - loss: 74.6781 - val_loss: 40.4125\n",
      "Epoch 122/450\n",
      "382/382 [==============================] - 0s 641us/step - loss: 76.7065 - val_loss: 69.3212\n",
      "Epoch 123/450\n",
      "382/382 [==============================] - 0s 644us/step - loss: 73.3123 - val_loss: 49.0691\n",
      "Epoch 124/450\n",
      "382/382 [==============================] - 0s 627us/step - loss: 69.3718 - val_loss: 69.7124\n",
      "Epoch 125/450\n",
      "382/382 [==============================] - 0s 649us/step - loss: 70.3657 - val_loss: 66.1465\n",
      "Epoch 126/450\n",
      "382/382 [==============================] - 0s 632us/step - loss: 73.7959 - val_loss: 57.1058\n",
      "Epoch 127/450\n",
      "382/382 [==============================] - 0s 608us/step - loss: 70.2126 - val_loss: 78.1070\n",
      "Epoch 128/450\n",
      "382/382 [==============================] - 0s 650us/step - loss: 77.2782 - val_loss: 60.4738\n",
      "Epoch 129/450\n",
      "382/382 [==============================] - 0s 653us/step - loss: 67.9379 - val_loss: 55.4503\n",
      "Epoch 130/450\n",
      "382/382 [==============================] - 0s 629us/step - loss: 69.5532 - val_loss: 56.0173\n",
      "Epoch 131/450\n",
      "382/382 [==============================] - 0s 653us/step - loss: 68.3213 - val_loss: 53.5812\n",
      "Epoch 132/450\n",
      "382/382 [==============================] - 0s 605us/step - loss: 68.1877 - val_loss: 50.3434\n",
      "Epoch 133/450\n",
      "382/382 [==============================] - 0s 667us/step - loss: 66.1702 - val_loss: 58.6100\n",
      "Epoch 134/450\n",
      "382/382 [==============================] - 0s 630us/step - loss: 64.9253 - val_loss: 50.7626\n",
      "Epoch 135/450\n",
      "382/382 [==============================] - 0s 628us/step - loss: 70.8938 - val_loss: 60.2970\n",
      "Epoch 136/450\n",
      "382/382 [==============================] - 0s 621us/step - loss: 65.4894 - val_loss: 71.3724\n",
      "Epoch 137/450\n",
      "382/382 [==============================] - 0s 652us/step - loss: 66.7428 - val_loss: 67.8609\n",
      "Epoch 138/450\n",
      "382/382 [==============================] - 0s 656us/step - loss: 68.1314 - val_loss: 85.6754\n",
      "Epoch 139/450\n",
      "382/382 [==============================] - 0s 624us/step - loss: 66.3296 - val_loss: 66.7690\n",
      "Epoch 140/450\n",
      "382/382 [==============================] - 0s 660us/step - loss: 65.8902 - val_loss: 93.9720\n",
      "Epoch 141/450\n",
      "382/382 [==============================] - 0s 625us/step - loss: 69.6678 - val_loss: 70.6651\n",
      "Epoch 142/450\n",
      "382/382 [==============================] - 0s 652us/step - loss: 65.9253 - val_loss: 48.7308\n",
      "Epoch 143/450\n",
      "382/382 [==============================] - 0s 658us/step - loss: 72.1049 - val_loss: 56.2857\n",
      "Epoch 144/450\n",
      "382/382 [==============================] - 0s 649us/step - loss: 64.6121 - val_loss: 60.2759\n",
      "Epoch 145/450\n",
      "382/382 [==============================] - 0s 650us/step - loss: 64.1550 - val_loss: 62.2215\n",
      "Epoch 146/450\n",
      "382/382 [==============================] - 0s 677us/step - loss: 68.8042 - val_loss: 47.5508\n",
      "Epoch 147/450\n",
      "382/382 [==============================] - 0s 636us/step - loss: 72.5483 - val_loss: 80.7848\n",
      "Epoch 148/450\n",
      "382/382 [==============================] - 0s 647us/step - loss: 67.9934 - val_loss: 60.9985\n",
      "Epoch 149/450\n",
      "382/382 [==============================] - 0s 675us/step - loss: 62.9578 - val_loss: 71.4375\n",
      "Epoch 150/450\n",
      "382/382 [==============================] - 0s 640us/step - loss: 62.0292 - val_loss: 75.2139\n",
      "Epoch 151/450\n",
      "382/382 [==============================] - 0s 642us/step - loss: 62.8909 - val_loss: 52.9768\n",
      "Epoch 152/450\n",
      "382/382 [==============================] - 0s 612us/step - loss: 68.2433 - val_loss: 61.5852\n",
      "Epoch 153/450\n",
      "382/382 [==============================] - 0s 619us/step - loss: 64.9803 - val_loss: 59.9182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/450\n",
      "382/382 [==============================] - 0s 623us/step - loss: 60.6324 - val_loss: 81.8881\n",
      "Epoch 155/450\n",
      "382/382 [==============================] - 0s 604us/step - loss: 62.8994 - val_loss: 70.0181\n",
      "Epoch 156/450\n",
      "382/382 [==============================] - 0s 648us/step - loss: 60.6305 - val_loss: 82.5653\n",
      "Epoch 157/450\n",
      "382/382 [==============================] - 0s 670us/step - loss: 62.3183 - val_loss: 59.8770\n",
      "Epoch 158/450\n",
      "382/382 [==============================] - 0s 626us/step - loss: 60.1927 - val_loss: 87.1789\n",
      "Epoch 159/450\n",
      "382/382 [==============================] - 0s 695us/step - loss: 68.4486 - val_loss: 55.7920\n",
      "Epoch 160/450\n",
      "382/382 [==============================] - 0s 669us/step - loss: 60.4468 - val_loss: 57.1170\n",
      "Epoch 161/450\n",
      "382/382 [==============================] - 0s 661us/step - loss: 64.2411 - val_loss: 66.9606\n",
      "Epoch 162/450\n",
      "382/382 [==============================] - 0s 659us/step - loss: 79.2798 - val_loss: 67.2703\n",
      "Epoch 163/450\n",
      "382/382 [==============================] - 0s 632us/step - loss: 63.7083 - val_loss: 63.8481\n",
      "Epoch 164/450\n",
      "382/382 [==============================] - 0s 657us/step - loss: 59.3493 - val_loss: 68.7138\n",
      "Epoch 165/450\n",
      "382/382 [==============================] - 0s 614us/step - loss: 59.3582 - val_loss: 81.8534\n",
      "Epoch 166/450\n",
      "382/382 [==============================] - 0s 625us/step - loss: 58.3864 - val_loss: 62.6636\n",
      "Epoch 167/450\n",
      "382/382 [==============================] - 0s 644us/step - loss: 57.2029 - val_loss: 77.3082\n",
      "Epoch 168/450\n",
      "382/382 [==============================] - 0s 609us/step - loss: 60.0887 - val_loss: 77.6339\n",
      "Epoch 169/450\n",
      "382/382 [==============================] - 0s 676us/step - loss: 57.1395 - val_loss: 66.9031\n",
      "Epoch 170/450\n",
      "382/382 [==============================] - 0s 659us/step - loss: 58.2225 - val_loss: 68.4536\n",
      "Epoch 171/450\n",
      "382/382 [==============================] - 0s 702us/step - loss: 60.9688 - val_loss: 44.9692\n",
      "Epoch 172/450\n",
      "382/382 [==============================] - 0s 615us/step - loss: 64.4449 - val_loss: 70.6090\n",
      "Epoch 173/450\n",
      "382/382 [==============================] - 0s 665us/step - loss: 56.7757 - val_loss: 55.8597\n",
      "Epoch 174/450\n",
      "382/382 [==============================] - 0s 659us/step - loss: 56.4511 - val_loss: 66.6940\n",
      "Epoch 175/450\n",
      "382/382 [==============================] - 0s 637us/step - loss: 58.4730 - val_loss: 47.2919\n",
      "Epoch 176/450\n",
      "382/382 [==============================] - 0s 640us/step - loss: 58.3760 - val_loss: 56.1299\n",
      "Epoch 177/450\n",
      "382/382 [==============================] - 0s 674us/step - loss: 57.8310 - val_loss: 49.9778\n",
      "Epoch 178/450\n",
      "382/382 [==============================] - 0s 644us/step - loss: 60.1766 - val_loss: 75.6127\n",
      "Epoch 179/450\n",
      "382/382 [==============================] - 0s 628us/step - loss: 57.4567 - val_loss: 74.7438\n",
      "Epoch 180/450\n",
      "382/382 [==============================] - 0s 658us/step - loss: 55.9661 - val_loss: 53.7508\n",
      "Epoch 181/450\n",
      "382/382 [==============================] - 0s 651us/step - loss: 55.3363 - val_loss: 78.7720\n",
      "Epoch 182/450\n",
      "382/382 [==============================] - 0s 651us/step - loss: 54.6372 - val_loss: 90.8457\n",
      "Epoch 183/450\n",
      "382/382 [==============================] - 0s 664us/step - loss: 55.7564 - val_loss: 79.3195\n",
      "Epoch 184/450\n",
      "382/382 [==============================] - 0s 652us/step - loss: 58.3749 - val_loss: 33.9167\n",
      "Epoch 185/450\n",
      "382/382 [==============================] - 0s 619us/step - loss: 66.6492 - val_loss: 61.9707\n",
      "Epoch 186/450\n",
      "382/382 [==============================] - 0s 657us/step - loss: 63.8671 - val_loss: 64.3248\n",
      "Epoch 187/450\n",
      "382/382 [==============================] - 0s 612us/step - loss: 59.9034 - val_loss: 64.4944\n",
      "Epoch 188/450\n",
      "382/382 [==============================] - 0s 596us/step - loss: 53.6172 - val_loss: 64.8720\n",
      "Epoch 189/450\n",
      "382/382 [==============================] - 0s 636us/step - loss: 55.4241 - val_loss: 65.8978\n",
      "Epoch 190/450\n",
      "382/382 [==============================] - 0s 606us/step - loss: 52.3385 - val_loss: 91.3632\n",
      "Epoch 191/450\n",
      "382/382 [==============================] - 0s 631us/step - loss: 55.5802 - val_loss: 88.7213\n",
      "Epoch 192/450\n",
      "382/382 [==============================] - 0s 621us/step - loss: 53.8528 - val_loss: 73.5814\n",
      "Epoch 193/450\n",
      "382/382 [==============================] - 0s 647us/step - loss: 50.3838 - val_loss: 53.8316\n",
      "Epoch 194/450\n",
      "382/382 [==============================] - 0s 679us/step - loss: 55.7571 - val_loss: 60.7882\n",
      "Epoch 195/450\n",
      "382/382 [==============================] - 0s 648us/step - loss: 56.6959 - val_loss: 76.1449\n",
      "Epoch 196/450\n",
      "382/382 [==============================] - 0s 647us/step - loss: 52.3052 - val_loss: 78.2573\n",
      "Epoch 197/450\n",
      "382/382 [==============================] - 0s 644us/step - loss: 53.0303 - val_loss: 91.3384\n",
      "Epoch 198/450\n",
      "382/382 [==============================] - 0s 658us/step - loss: 49.4378 - val_loss: 66.2836\n",
      "Epoch 199/450\n",
      "382/382 [==============================] - 0s 602us/step - loss: 56.5380 - val_loss: 76.0322\n",
      "Epoch 200/450\n",
      "382/382 [==============================] - 0s 673us/step - loss: 54.1997 - val_loss: 104.5940\n",
      "Epoch 201/450\n",
      "382/382 [==============================] - 0s 637us/step - loss: 53.6988 - val_loss: 74.0366\n",
      "Epoch 202/450\n",
      "382/382 [==============================] - 0s 653us/step - loss: 56.3352 - val_loss: 49.5023\n",
      "Epoch 203/450\n",
      "382/382 [==============================] - 0s 684us/step - loss: 53.4988 - val_loss: 72.0764\n",
      "Epoch 204/450\n",
      "382/382 [==============================] - 0s 644us/step - loss: 48.7212 - val_loss: 78.6612\n",
      "Epoch 205/450\n",
      "382/382 [==============================] - 0s 639us/step - loss: 51.1042 - val_loss: 79.0171\n",
      "Epoch 206/450\n",
      "382/382 [==============================] - 0s 636us/step - loss: 58.1781 - val_loss: 62.3283\n",
      "Epoch 207/450\n",
      "382/382 [==============================] - 0s 634us/step - loss: 54.4496 - val_loss: 97.1890\n",
      "Epoch 208/450\n",
      "382/382 [==============================] - 0s 652us/step - loss: 48.7471 - val_loss: 69.1300\n",
      "Epoch 209/450\n",
      "382/382 [==============================] - 0s 626us/step - loss: 51.7006 - val_loss: 126.8872\n",
      "Epoch 210/450\n",
      "382/382 [==============================] - 0s 677us/step - loss: 61.7157 - val_loss: 72.8349\n",
      "Epoch 211/450\n",
      "382/382 [==============================] - 0s 668us/step - loss: 49.7087 - val_loss: 94.2646\n",
      "Epoch 212/450\n",
      "382/382 [==============================] - 0s 603us/step - loss: 47.5764 - val_loss: 82.6107\n",
      "Epoch 213/450\n",
      "382/382 [==============================] - 0s 680us/step - loss: 54.8463 - val_loss: 64.9763\n",
      "Epoch 214/450\n",
      "382/382 [==============================] - 0s 643us/step - loss: 46.6532 - val_loss: 69.8203\n",
      "Epoch 215/450\n",
      "382/382 [==============================] - 0s 643us/step - loss: 47.3128 - val_loss: 68.4210\n",
      "Epoch 216/450\n",
      "382/382 [==============================] - 0s 707us/step - loss: 48.6683 - val_loss: 81.4657\n",
      "Epoch 217/450\n",
      "382/382 [==============================] - 0s 658us/step - loss: 56.0454 - val_loss: 90.6210\n",
      "Epoch 218/450\n",
      "382/382 [==============================] - 0s 650us/step - loss: 71.3890 - val_loss: 41.5024\n",
      "Epoch 219/450\n",
      "382/382 [==============================] - 0s 675us/step - loss: 51.7384 - val_loss: 87.0777\n",
      "Epoch 220/450\n",
      "382/382 [==============================] - 0s 671us/step - loss: 52.1196 - val_loss: 100.2367\n",
      "Epoch 221/450\n",
      "382/382 [==============================] - 0s 639us/step - loss: 48.5006 - val_loss: 80.9205\n",
      "Epoch 222/450\n",
      "382/382 [==============================] - 0s 629us/step - loss: 48.8157 - val_loss: 84.7109\n",
      "Epoch 223/450\n",
      "382/382 [==============================] - 0s 639us/step - loss: 46.5146 - val_loss: 56.1733\n",
      "Epoch 224/450\n",
      "382/382 [==============================] - 0s 617us/step - loss: 47.6991 - val_loss: 81.9691\n",
      "Epoch 225/450\n",
      "382/382 [==============================] - 0s 654us/step - loss: 46.4065 - val_loss: 80.3988\n",
      "Epoch 226/450\n",
      "382/382 [==============================] - 0s 665us/step - loss: 46.8081 - val_loss: 75.0919\n",
      "Epoch 227/450\n",
      "382/382 [==============================] - 0s 666us/step - loss: 51.1685 - val_loss: 88.0873\n",
      "Epoch 228/450\n",
      "382/382 [==============================] - 0s 620us/step - loss: 44.4903 - val_loss: 64.4788\n",
      "Epoch 229/450\n",
      "382/382 [==============================] - 0s 675us/step - loss: 43.1859 - val_loss: 85.1444\n",
      "Epoch 230/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 685us/step - loss: 46.6402 - val_loss: 85.2261\n",
      "Epoch 231/450\n",
      "382/382 [==============================] - 0s 652us/step - loss: 42.8592 - val_loss: 63.2680\n",
      "Epoch 232/450\n",
      "382/382 [==============================] - 0s 649us/step - loss: 46.9659 - val_loss: 62.7950\n",
      "Epoch 233/450\n",
      "382/382 [==============================] - 0s 682us/step - loss: 52.2079 - val_loss: 103.5669\n",
      "Epoch 234/450\n",
      "382/382 [==============================] - 0s 651us/step - loss: 47.7092 - val_loss: 69.3022\n",
      "Epoch 235/450\n",
      "382/382 [==============================] - 0s 659us/step - loss: 47.3142 - val_loss: 78.7165\n",
      "Epoch 236/450\n",
      "382/382 [==============================] - 0s 654us/step - loss: 44.8277 - val_loss: 84.7294\n",
      "Epoch 237/450\n",
      "382/382 [==============================] - 0s 648us/step - loss: 46.2948 - val_loss: 128.9995\n",
      "Epoch 238/450\n",
      "382/382 [==============================] - 0s 649us/step - loss: 44.7980 - val_loss: 82.6618\n",
      "Epoch 239/450\n",
      "382/382 [==============================] - 0s 653us/step - loss: 44.9617 - val_loss: 87.2148\n",
      "Epoch 240/450\n",
      "382/382 [==============================] - 0s 580us/step - loss: 47.9961 - val_loss: 115.0768\n",
      "Epoch 241/450\n",
      "382/382 [==============================] - 0s 635us/step - loss: 51.1324 - val_loss: 105.5650\n",
      "Epoch 242/450\n",
      "382/382 [==============================] - 0s 640us/step - loss: 47.7227 - val_loss: 85.9467\n",
      "Epoch 243/450\n",
      "382/382 [==============================] - 0s 612us/step - loss: 46.9570 - val_loss: 146.9325\n",
      "Epoch 244/450\n",
      "382/382 [==============================] - 0s 617us/step - loss: 49.2491 - val_loss: 111.9064\n",
      "Epoch 245/450\n",
      "382/382 [==============================] - 0s 649us/step - loss: 44.1761 - val_loss: 121.9940\n",
      "Epoch 246/450\n",
      "382/382 [==============================] - 0s 641us/step - loss: 44.4515 - val_loss: 84.2728\n",
      "Epoch 247/450\n",
      "382/382 [==============================] - 0s 633us/step - loss: 41.4514 - val_loss: 109.2043\n",
      "Epoch 248/450\n",
      "382/382 [==============================] - 0s 636us/step - loss: 45.8370 - val_loss: 124.6580\n",
      "Epoch 249/450\n",
      "382/382 [==============================] - 0s 664us/step - loss: 41.5870 - val_loss: 105.9605\n",
      "Epoch 250/450\n",
      "382/382 [==============================] - 0s 640us/step - loss: 43.1048 - val_loss: 70.5921\n",
      "Epoch 251/450\n",
      "382/382 [==============================] - 0s 677us/step - loss: 43.7191 - val_loss: 93.3673\n",
      "Epoch 252/450\n",
      "382/382 [==============================] - 0s 642us/step - loss: 41.7511 - val_loss: 73.0100\n",
      "Epoch 253/450\n",
      "382/382 [==============================] - 0s 659us/step - loss: 42.8176 - val_loss: 74.7223\n",
      "Epoch 254/450\n",
      "382/382 [==============================] - 0s 660us/step - loss: 49.4428 - val_loss: 63.7904\n",
      "Epoch 255/450\n",
      "382/382 [==============================] - 0s 650us/step - loss: 44.2059 - val_loss: 86.7294\n",
      "Epoch 256/450\n",
      "382/382 [==============================] - 0s 633us/step - loss: 45.3784 - val_loss: 92.7892\n",
      "Epoch 257/450\n",
      "382/382 [==============================] - 0s 617us/step - loss: 47.2415 - val_loss: 90.2301\n",
      "Epoch 258/450\n",
      "382/382 [==============================] - 0s 627us/step - loss: 47.7362 - val_loss: 117.0580\n",
      "Epoch 259/450\n",
      "382/382 [==============================] - 0s 670us/step - loss: 47.7050 - val_loss: 73.5766\n",
      "Epoch 260/450\n",
      "382/382 [==============================] - 0s 667us/step - loss: 43.7299 - val_loss: 78.4548\n",
      "Epoch 261/450\n",
      "382/382 [==============================] - 0s 665us/step - loss: 41.7864 - val_loss: 89.4458\n",
      "Epoch 262/450\n",
      "382/382 [==============================] - 0s 618us/step - loss: 38.3776 - val_loss: 104.6557\n",
      "Epoch 263/450\n",
      "382/382 [==============================] - 0s 622us/step - loss: 41.1124 - val_loss: 69.3999\n",
      "Epoch 264/450\n",
      "382/382 [==============================] - 0s 653us/step - loss: 40.2741 - val_loss: 128.0088\n",
      "Epoch 265/450\n",
      "382/382 [==============================] - 0s 633us/step - loss: 41.0946 - val_loss: 78.4262\n",
      "Epoch 266/450\n",
      "382/382 [==============================] - 0s 670us/step - loss: 41.2691 - val_loss: 76.2317\n",
      "Epoch 267/450\n",
      "382/382 [==============================] - 0s 659us/step - loss: 42.4148 - val_loss: 105.5145\n",
      "Epoch 268/450\n",
      "382/382 [==============================] - 0s 679us/step - loss: 38.9388 - val_loss: 99.6610\n",
      "Epoch 269/450\n",
      "382/382 [==============================] - 0s 660us/step - loss: 40.8972 - val_loss: 57.6017\n",
      "Epoch 270/450\n",
      "382/382 [==============================] - 0s 632us/step - loss: 41.4292 - val_loss: 78.4458\n",
      "Epoch 271/450\n",
      "382/382 [==============================] - 0s 652us/step - loss: 42.3450 - val_loss: 81.7987\n",
      "Epoch 272/450\n",
      "382/382 [==============================] - 0s 639us/step - loss: 36.7869 - val_loss: 78.8608\n",
      "Epoch 273/450\n",
      "382/382 [==============================] - 0s 605us/step - loss: 39.5310 - val_loss: 81.2357\n",
      "Epoch 274/450\n",
      "382/382 [==============================] - 0s 632us/step - loss: 40.3866 - val_loss: 110.9803\n",
      "Epoch 275/450\n",
      "382/382 [==============================] - 0s 668us/step - loss: 37.5938 - val_loss: 80.1913\n",
      "Epoch 276/450\n",
      "382/382 [==============================] - 0s 628us/step - loss: 39.7740 - val_loss: 115.8749\n",
      "Epoch 277/450\n",
      "382/382 [==============================] - 0s 639us/step - loss: 39.8123 - val_loss: 80.9073\n",
      "Epoch 278/450\n",
      "382/382 [==============================] - 0s 635us/step - loss: 36.6279 - val_loss: 135.4275\n",
      "Epoch 279/450\n",
      "382/382 [==============================] - 0s 636us/step - loss: 39.5669 - val_loss: 97.4093\n",
      "Epoch 280/450\n",
      "382/382 [==============================] - 0s 624us/step - loss: 37.0008 - val_loss: 92.1531\n",
      "Epoch 281/450\n",
      "382/382 [==============================] - 0s 631us/step - loss: 44.6804 - val_loss: 102.7443\n",
      "Epoch 282/450\n",
      "382/382 [==============================] - 0s 663us/step - loss: 36.8045 - val_loss: 79.1253\n",
      "Epoch 283/450\n",
      "382/382 [==============================] - 0s 616us/step - loss: 38.0202 - val_loss: 97.0841\n",
      "Epoch 284/450\n",
      "382/382 [==============================] - 0s 657us/step - loss: 37.6095 - val_loss: 128.8077\n",
      "Epoch 285/450\n",
      "382/382 [==============================] - 0s 644us/step - loss: 37.0782 - val_loss: 72.7478\n",
      "Epoch 286/450\n",
      "382/382 [==============================] - 0s 664us/step - loss: 43.7485 - val_loss: 104.2598\n",
      "Epoch 287/450\n",
      "382/382 [==============================] - 0s 651us/step - loss: 36.1864 - val_loss: 116.4406\n",
      "Epoch 288/450\n",
      "382/382 [==============================] - 0s 639us/step - loss: 36.7627 - val_loss: 98.2545\n",
      "Epoch 289/450\n",
      "382/382 [==============================] - 0s 661us/step - loss: 34.6224 - val_loss: 123.9053\n",
      "Epoch 290/450\n",
      "382/382 [==============================] - 0s 634us/step - loss: 39.2403 - val_loss: 108.6000\n",
      "Epoch 291/450\n",
      "382/382 [==============================] - 0s 684us/step - loss: 47.2194 - val_loss: 68.5834\n",
      "Epoch 292/450\n",
      "382/382 [==============================] - 0s 604us/step - loss: 42.8195 - val_loss: 121.0922\n",
      "Epoch 293/450\n",
      "382/382 [==============================] - 0s 642us/step - loss: 36.8903 - val_loss: 116.1626\n",
      "Epoch 294/450\n",
      "382/382 [==============================] - 0s 648us/step - loss: 36.5876 - val_loss: 73.9207\n",
      "Epoch 295/450\n",
      "382/382 [==============================] - 0s 615us/step - loss: 36.2408 - val_loss: 107.4555\n",
      "Epoch 296/450\n",
      "382/382 [==============================] - 0s 613us/step - loss: 34.7067 - val_loss: 96.1380\n",
      "Epoch 297/450\n",
      "382/382 [==============================] - 0s 655us/step - loss: 35.6753 - val_loss: 127.4382\n",
      "Epoch 298/450\n",
      "382/382 [==============================] - 0s 652us/step - loss: 36.2018 - val_loss: 108.7154\n",
      "Epoch 299/450\n",
      "382/382 [==============================] - 0s 671us/step - loss: 35.5507 - val_loss: 121.7348\n",
      "Epoch 300/450\n",
      "382/382 [==============================] - 0s 643us/step - loss: 36.5035 - val_loss: 62.9493\n",
      "Epoch 301/450\n",
      "382/382 [==============================] - 0s 649us/step - loss: 39.0704 - val_loss: 102.0283\n",
      "Epoch 302/450\n",
      "382/382 [==============================] - 0s 651us/step - loss: 35.6959 - val_loss: 99.1060\n",
      "Epoch 303/450\n",
      "382/382 [==============================] - 0s 615us/step - loss: 37.9633 - val_loss: 90.1099\n",
      "Epoch 304/450\n",
      "382/382 [==============================] - 0s 638us/step - loss: 35.6995 - val_loss: 67.2433\n",
      "Epoch 305/450\n",
      "382/382 [==============================] - 0s 676us/step - loss: 36.7332 - val_loss: 70.6536\n",
      "Epoch 306/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 656us/step - loss: 35.5764 - val_loss: 81.2552\n",
      "Epoch 307/450\n",
      "382/382 [==============================] - 0s 672us/step - loss: 36.5424 - val_loss: 112.8047\n",
      "Epoch 308/450\n",
      "382/382 [==============================] - 0s 675us/step - loss: 34.2087 - val_loss: 91.9154\n",
      "Epoch 309/450\n",
      "382/382 [==============================] - 0s 629us/step - loss: 34.7455 - val_loss: 76.2640\n",
      "Epoch 310/450\n",
      "382/382 [==============================] - 0s 669us/step - loss: 32.5403 - val_loss: 138.7852\n",
      "Epoch 311/450\n",
      "382/382 [==============================] - 0s 642us/step - loss: 32.9289 - val_loss: 129.8810\n",
      "Epoch 312/450\n",
      "382/382 [==============================] - 0s 650us/step - loss: 33.8725 - val_loss: 90.7664\n",
      "Epoch 313/450\n",
      "382/382 [==============================] - 0s 669us/step - loss: 34.9779 - val_loss: 141.8184\n",
      "Epoch 314/450\n",
      "382/382 [==============================] - 0s 641us/step - loss: 36.1486 - val_loss: 127.2793\n",
      "Epoch 315/450\n",
      "382/382 [==============================] - 0s 664us/step - loss: 33.9121 - val_loss: 83.3725\n",
      "Epoch 316/450\n",
      "382/382 [==============================] - 0s 615us/step - loss: 30.9598 - val_loss: 83.9952\n",
      "Epoch 317/450\n",
      "382/382 [==============================] - 0s 656us/step - loss: 30.7914 - val_loss: 129.6676\n",
      "Epoch 318/450\n",
      "382/382 [==============================] - 0s 639us/step - loss: 38.6345 - val_loss: 108.6092\n",
      "Epoch 319/450\n",
      "382/382 [==============================] - 0s 660us/step - loss: 34.5749 - val_loss: 124.3319\n",
      "Epoch 320/450\n",
      "382/382 [==============================] - 0s 660us/step - loss: 30.8682 - val_loss: 93.1779\n",
      "Epoch 321/450\n",
      "382/382 [==============================] - 0s 620us/step - loss: 32.0965 - val_loss: 70.6449\n",
      "Epoch 322/450\n",
      "382/382 [==============================] - 0s 678us/step - loss: 33.1188 - val_loss: 60.9636\n",
      "Epoch 323/450\n",
      "382/382 [==============================] - 0s 607us/step - loss: 35.1269 - val_loss: 96.2872\n",
      "Epoch 324/450\n",
      "382/382 [==============================] - 0s 615us/step - loss: 34.4830 - val_loss: 80.1726\n",
      "Epoch 325/450\n",
      "382/382 [==============================] - 0s 655us/step - loss: 32.4010 - val_loss: 76.5341\n",
      "Epoch 326/450\n",
      "382/382 [==============================] - 0s 642us/step - loss: 32.8974 - val_loss: 159.3987\n",
      "Epoch 327/450\n",
      "382/382 [==============================] - 0s 694us/step - loss: 32.8316 - val_loss: 113.6661\n",
      "Epoch 328/450\n",
      "382/382 [==============================] - 0s 656us/step - loss: 31.6245 - val_loss: 111.7360\n",
      "Epoch 329/450\n",
      "382/382 [==============================] - 0s 674us/step - loss: 32.9070 - val_loss: 98.6161\n",
      "Epoch 330/450\n",
      "382/382 [==============================] - 0s 651us/step - loss: 31.6039 - val_loss: 90.8397\n",
      "Epoch 331/450\n",
      "382/382 [==============================] - 0s 659us/step - loss: 31.7379 - val_loss: 61.4432\n",
      "Epoch 332/450\n",
      "382/382 [==============================] - 0s 665us/step - loss: 31.5707 - val_loss: 106.9689\n",
      "Epoch 333/450\n",
      "382/382 [==============================] - 0s 664us/step - loss: 32.9319 - val_loss: 113.9595\n",
      "Epoch 334/450\n",
      "382/382 [==============================] - 0s 637us/step - loss: 31.3624 - val_loss: 84.8988\n",
      "Epoch 335/450\n",
      "382/382 [==============================] - 0s 695us/step - loss: 33.3896 - val_loss: 149.6134\n",
      "Epoch 336/450\n",
      "382/382 [==============================] - 0s 594us/step - loss: 48.0998 - val_loss: 79.7227\n",
      "Epoch 337/450\n",
      "382/382 [==============================] - 0s 645us/step - loss: 31.0532 - val_loss: 111.7174\n",
      "Epoch 338/450\n",
      "382/382 [==============================] - 0s 620us/step - loss: 28.9167 - val_loss: 123.3974\n",
      "Epoch 339/450\n",
      "382/382 [==============================] - 0s 649us/step - loss: 30.3622 - val_loss: 105.6259\n",
      "Epoch 340/450\n",
      "382/382 [==============================] - 0s 642us/step - loss: 29.3308 - val_loss: 94.4144\n",
      "Epoch 341/450\n",
      "382/382 [==============================] - 0s 654us/step - loss: 30.4596 - val_loss: 116.2349\n",
      "Epoch 342/450\n",
      "382/382 [==============================] - 0s 638us/step - loss: 29.5469 - val_loss: 109.3525\n",
      "Epoch 343/450\n",
      "382/382 [==============================] - 0s 648us/step - loss: 29.6619 - val_loss: 163.9984\n",
      "Epoch 344/450\n",
      "382/382 [==============================] - 0s 640us/step - loss: 30.0075 - val_loss: 134.5591\n",
      "Epoch 345/450\n",
      "382/382 [==============================] - 0s 671us/step - loss: 28.9378 - val_loss: 88.4603\n",
      "Epoch 346/450\n",
      "382/382 [==============================] - 0s 655us/step - loss: 29.3655 - val_loss: 123.8941\n",
      "Epoch 347/450\n",
      "382/382 [==============================] - 0s 653us/step - loss: 29.3054 - val_loss: 125.5618\n",
      "Epoch 348/450\n",
      "382/382 [==============================] - 0s 673us/step - loss: 31.6555 - val_loss: 127.1553\n",
      "Epoch 349/450\n",
      "382/382 [==============================] - 0s 631us/step - loss: 26.3035 - val_loss: 78.4350\n",
      "Epoch 350/450\n",
      "382/382 [==============================] - 0s 638us/step - loss: 28.2469 - val_loss: 95.0840\n",
      "Epoch 351/450\n",
      "382/382 [==============================] - 0s 689us/step - loss: 28.9406 - val_loss: 81.5333\n",
      "Epoch 352/450\n",
      "382/382 [==============================] - 0s 631us/step - loss: 27.1539 - val_loss: 118.8709\n",
      "Epoch 353/450\n",
      "382/382 [==============================] - 0s 658us/step - loss: 30.2598 - val_loss: 125.6842\n",
      "Epoch 354/450\n",
      "382/382 [==============================] - 0s 663us/step - loss: 52.0600 - val_loss: 119.0248\n",
      "Epoch 355/450\n",
      "382/382 [==============================] - 0s 617us/step - loss: 27.4692 - val_loss: 82.6152\n",
      "Epoch 356/450\n",
      "382/382 [==============================] - 0s 654us/step - loss: 30.3794 - val_loss: 111.6963\n",
      "Epoch 357/450\n",
      "382/382 [==============================] - 0s 645us/step - loss: 27.8462 - val_loss: 117.0810\n",
      "Epoch 358/450\n",
      "382/382 [==============================] - 0s 671us/step - loss: 30.6699 - val_loss: 83.2503\n",
      "Epoch 359/450\n",
      "382/382 [==============================] - 0s 622us/step - loss: 31.4684 - val_loss: 163.9949\n",
      "Epoch 360/450\n",
      "382/382 [==============================] - 0s 626us/step - loss: 27.7267 - val_loss: 79.7622\n",
      "Epoch 361/450\n",
      "382/382 [==============================] - 0s 660us/step - loss: 26.9852 - val_loss: 87.9099\n",
      "Epoch 362/450\n",
      "382/382 [==============================] - 0s 643us/step - loss: 27.7517 - val_loss: 102.7450\n",
      "Epoch 363/450\n",
      "382/382 [==============================] - 0s 619us/step - loss: 30.3921 - val_loss: 109.5564\n",
      "Epoch 364/450\n",
      "382/382 [==============================] - 0s 637us/step - loss: 30.2040 - val_loss: 117.0190\n",
      "Epoch 365/450\n",
      "382/382 [==============================] - 0s 620us/step - loss: 25.8436 - val_loss: 95.7773\n",
      "Epoch 366/450\n",
      "382/382 [==============================] - 0s 658us/step - loss: 25.0489 - val_loss: 93.3874\n",
      "Epoch 367/450\n",
      "382/382 [==============================] - 0s 678us/step - loss: 27.1661 - val_loss: 83.1026\n",
      "Epoch 368/450\n",
      "382/382 [==============================] - 0s 679us/step - loss: 32.9964 - val_loss: 126.5566\n",
      "Epoch 369/450\n",
      "382/382 [==============================] - 0s 622us/step - loss: 28.3699 - val_loss: 165.5652\n",
      "Epoch 370/450\n",
      "382/382 [==============================] - 0s 684us/step - loss: 43.4395 - val_loss: 102.3282\n",
      "Epoch 371/450\n",
      "382/382 [==============================] - 0s 629us/step - loss: 31.5167 - val_loss: 117.4817\n",
      "Epoch 372/450\n",
      "382/382 [==============================] - 0s 629us/step - loss: 28.4179 - val_loss: 115.4453\n",
      "Epoch 373/450\n",
      "382/382 [==============================] - 0s 647us/step - loss: 28.0710 - val_loss: 150.3733\n",
      "Epoch 374/450\n",
      "382/382 [==============================] - 0s 630us/step - loss: 28.5763 - val_loss: 103.7854\n",
      "Epoch 375/450\n",
      "382/382 [==============================] - 0s 701us/step - loss: 25.5942 - val_loss: 131.5037\n",
      "Epoch 376/450\n",
      "382/382 [==============================] - 0s 714us/step - loss: 27.6939 - val_loss: 67.7301\n",
      "Epoch 377/450\n",
      "382/382 [==============================] - 0s 648us/step - loss: 28.4254 - val_loss: 135.0862\n",
      "Epoch 378/450\n",
      "382/382 [==============================] - 0s 629us/step - loss: 26.3322 - val_loss: 78.9214\n",
      "Epoch 379/450\n",
      "382/382 [==============================] - 0s 656us/step - loss: 24.3012 - val_loss: 118.8053\n",
      "Epoch 380/450\n",
      "382/382 [==============================] - 0s 632us/step - loss: 26.2123 - val_loss: 171.6784\n",
      "Epoch 381/450\n",
      "382/382 [==============================] - 0s 620us/step - loss: 32.4009 - val_loss: 91.7581\n",
      "Epoch 382/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 668us/step - loss: 28.3805 - val_loss: 124.7814\n",
      "Epoch 383/450\n",
      "382/382 [==============================] - 0s 659us/step - loss: 24.7436 - val_loss: 119.8811\n",
      "Epoch 384/450\n",
      "382/382 [==============================] - 0s 636us/step - loss: 26.0634 - val_loss: 143.2291\n",
      "Epoch 385/450\n",
      "382/382 [==============================] - 0s 614us/step - loss: 25.4200 - val_loss: 139.7395\n",
      "Epoch 386/450\n",
      "382/382 [==============================] - 0s 653us/step - loss: 27.7948 - val_loss: 147.7657\n",
      "Epoch 387/450\n",
      "382/382 [==============================] - 0s 622us/step - loss: 24.0403 - val_loss: 101.2268\n",
      "Epoch 388/450\n",
      "382/382 [==============================] - 0s 638us/step - loss: 25.2971 - val_loss: 130.3694\n",
      "Epoch 389/450\n",
      "382/382 [==============================] - 0s 648us/step - loss: 32.3363 - val_loss: 160.5956\n",
      "Epoch 390/450\n",
      "382/382 [==============================] - 0s 654us/step - loss: 27.9329 - val_loss: 142.4141\n",
      "Epoch 391/450\n",
      "382/382 [==============================] - 0s 612us/step - loss: 25.1812 - val_loss: 165.8722\n",
      "Epoch 392/450\n",
      "382/382 [==============================] - 0s 651us/step - loss: 25.5240 - val_loss: 144.7802\n",
      "Epoch 393/450\n",
      "382/382 [==============================] - 0s 629us/step - loss: 25.7197 - val_loss: 140.7541\n",
      "Epoch 394/450\n",
      "382/382 [==============================] - 0s 628us/step - loss: 25.3883 - val_loss: 119.3494\n",
      "Epoch 395/450\n",
      "382/382 [==============================] - 0s 642us/step - loss: 23.4384 - val_loss: 107.9448\n",
      "Epoch 396/450\n",
      "382/382 [==============================] - 0s 669us/step - loss: 23.3777 - val_loss: 125.7011\n",
      "Epoch 397/450\n",
      "382/382 [==============================] - 0s 614us/step - loss: 30.4661 - val_loss: 104.5990\n",
      "Epoch 398/450\n",
      "382/382 [==============================] - 0s 613us/step - loss: 41.6362 - val_loss: 90.2365\n",
      "Epoch 399/450\n",
      "382/382 [==============================] - 0s 668us/step - loss: 27.3051 - val_loss: 102.4025\n",
      "Epoch 400/450\n",
      "382/382 [==============================] - 0s 622us/step - loss: 23.5239 - val_loss: 139.0407\n",
      "Epoch 401/450\n",
      "382/382 [==============================] - 0s 656us/step - loss: 21.6944 - val_loss: 93.6545\n",
      "Epoch 402/450\n",
      "382/382 [==============================] - 0s 621us/step - loss: 25.3047 - val_loss: 125.1540\n",
      "Epoch 403/450\n",
      "382/382 [==============================] - 0s 687us/step - loss: 25.7288 - val_loss: 155.3018\n",
      "Epoch 404/450\n",
      "382/382 [==============================] - 0s 625us/step - loss: 21.9480 - val_loss: 93.4276\n",
      "Epoch 405/450\n",
      "382/382 [==============================] - 0s 631us/step - loss: 22.3860 - val_loss: 143.5036\n",
      "Epoch 406/450\n",
      "382/382 [==============================] - 0s 668us/step - loss: 22.8547 - val_loss: 103.6227\n",
      "Epoch 407/450\n",
      "382/382 [==============================] - 0s 604us/step - loss: 24.1156 - val_loss: 135.4433\n",
      "Epoch 408/450\n",
      "382/382 [==============================] - 0s 608us/step - loss: 26.1820 - val_loss: 92.4995\n",
      "Epoch 409/450\n",
      "382/382 [==============================] - 0s 643us/step - loss: 28.1675 - val_loss: 129.5518\n",
      "Epoch 410/450\n",
      "382/382 [==============================] - 0s 691us/step - loss: 24.8655 - val_loss: 121.7216\n",
      "Epoch 411/450\n",
      "382/382 [==============================] - 0s 633us/step - loss: 20.0214 - val_loss: 109.8293\n",
      "Epoch 412/450\n",
      "382/382 [==============================] - 0s 612us/step - loss: 20.6981 - val_loss: 101.6388\n",
      "Epoch 413/450\n",
      "382/382 [==============================] - 0s 645us/step - loss: 24.8841 - val_loss: 127.3255\n",
      "Epoch 414/450\n",
      "382/382 [==============================] - 0s 643us/step - loss: 20.7650 - val_loss: 115.5788\n",
      "Epoch 415/450\n",
      "382/382 [==============================] - 0s 667us/step - loss: 27.1263 - val_loss: 140.9937\n",
      "Epoch 416/450\n",
      "382/382 [==============================] - 0s 608us/step - loss: 23.9506 - val_loss: 114.2372\n",
      "Epoch 417/450\n",
      "382/382 [==============================] - 0s 603us/step - loss: 22.6693 - val_loss: 142.5352\n",
      "Epoch 418/450\n",
      "382/382 [==============================] - 0s 629us/step - loss: 24.8896 - val_loss: 178.1208\n",
      "Epoch 419/450\n",
      "382/382 [==============================] - 0s 643us/step - loss: 26.5475 - val_loss: 130.6626\n",
      "Epoch 420/450\n",
      "382/382 [==============================] - 0s 628us/step - loss: 24.9776 - val_loss: 62.1539\n",
      "Epoch 421/450\n",
      "382/382 [==============================] - 0s 655us/step - loss: 25.4797 - val_loss: 129.4026\n",
      "Epoch 422/450\n",
      "382/382 [==============================] - 0s 604us/step - loss: 24.2045 - val_loss: 142.3642\n",
      "Epoch 423/450\n",
      "382/382 [==============================] - 0s 621us/step - loss: 21.1749 - val_loss: 132.4333\n",
      "Epoch 424/450\n",
      "382/382 [==============================] - 0s 654us/step - loss: 22.9263 - val_loss: 115.5102\n",
      "Epoch 425/450\n",
      "382/382 [==============================] - 0s 617us/step - loss: 22.5431 - val_loss: 132.7135\n",
      "Epoch 426/450\n",
      "382/382 [==============================] - 0s 663us/step - loss: 23.7096 - val_loss: 92.9348\n",
      "Epoch 427/450\n",
      "382/382 [==============================] - 0s 628us/step - loss: 22.7425 - val_loss: 108.7033\n",
      "Epoch 428/450\n",
      "382/382 [==============================] - 0s 645us/step - loss: 21.1589 - val_loss: 125.4358\n",
      "Epoch 429/450\n",
      "382/382 [==============================] - 0s 606us/step - loss: 27.2956 - val_loss: 113.0819\n",
      "Epoch 430/450\n",
      "382/382 [==============================] - 0s 619us/step - loss: 21.9748 - val_loss: 120.6079\n",
      "Epoch 431/450\n",
      "382/382 [==============================] - 0s 661us/step - loss: 24.8134 - val_loss: 102.5833\n",
      "Epoch 432/450\n",
      "382/382 [==============================] - 0s 646us/step - loss: 22.2860 - val_loss: 130.2033\n",
      "Epoch 433/450\n",
      "382/382 [==============================] - 0s 660us/step - loss: 21.0545 - val_loss: 144.8142\n",
      "Epoch 434/450\n",
      "382/382 [==============================] - 0s 653us/step - loss: 20.5672 - val_loss: 126.7566\n",
      "Epoch 435/450\n",
      "382/382 [==============================] - 0s 674us/step - loss: 19.7583 - val_loss: 97.7451\n",
      "Epoch 436/450\n",
      "382/382 [==============================] - 0s 630us/step - loss: 20.4964 - val_loss: 109.3972\n",
      "Epoch 437/450\n",
      "382/382 [==============================] - 0s 608us/step - loss: 20.9143 - val_loss: 100.4080\n",
      "Epoch 438/450\n",
      "382/382 [==============================] - 0s 676us/step - loss: 21.3605 - val_loss: 171.4138\n",
      "Epoch 439/450\n",
      "382/382 [==============================] - 0s 625us/step - loss: 22.6071 - val_loss: 119.8056\n",
      "Epoch 440/450\n",
      "382/382 [==============================] - 0s 639us/step - loss: 20.3742 - val_loss: 93.7920\n",
      "Epoch 441/450\n",
      "382/382 [==============================] - 0s 660us/step - loss: 20.6215 - val_loss: 169.3988\n",
      "Epoch 442/450\n",
      "382/382 [==============================] - 0s 631us/step - loss: 19.6985 - val_loss: 79.7876\n",
      "Epoch 443/450\n",
      "382/382 [==============================] - 0s 642us/step - loss: 21.7010 - val_loss: 75.9097\n",
      "Epoch 444/450\n",
      "382/382 [==============================] - 0s 683us/step - loss: 35.3413 - val_loss: 186.1706\n",
      "Epoch 445/450\n",
      "382/382 [==============================] - 0s 640us/step - loss: 32.8999 - val_loss: 143.1008\n",
      "Epoch 446/450\n",
      "382/382 [==============================] - 0s 626us/step - loss: 23.9666 - val_loss: 127.9820\n",
      "Epoch 447/450\n",
      "382/382 [==============================] - 0s 701us/step - loss: 21.1893 - val_loss: 135.6593\n",
      "Epoch 448/450\n",
      "382/382 [==============================] - 0s 605us/step - loss: 21.1114 - val_loss: 138.8542\n",
      "Epoch 449/450\n",
      "382/382 [==============================] - 0s 629us/step - loss: 20.2665 - val_loss: 135.7654\n",
      "Epoch 450/450\n",
      "382/382 [==============================] - 0s 596us/step - loss: 21.2339 - val_loss: 137.2465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c2c2a41d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('> Data Loaded. Compiling...')\n",
    "epochs  = 450\n",
    "model = build_model([1, 50, 100, 1])\n",
    "history = LossHistory()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=250, nb_epoch=epochs, validation_split=0.05, callbacks=[history])\n",
    "\n",
    "#predictions = lstm.predict_sequences_multiple(model, X_test, seq_len, 50)\n",
    "#predicted = lstm.predict_sequence_full(model, X_test, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = predict_point_by_point(model, X_test)        \n",
    "predicted_train = predict_point_by_point(model, X_train)\n",
    "print(predicted_train.shape)\n",
    "print('Training duration (s) : ', time.time() - global_start_time)\n",
    "plot_results(predicted_train[:50], y_train[:50], False)\n",
    "plot_results(np.concatenate([predicted_train,predicted]), np.concatenate([y_train,y_test]), True)\n",
    "plot_results(predicted[:50], y_test[:50], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110.07462, 161.54561, 117.93207, 139.72617, 109.40031, 147.70137, 131.13268, 98.92771, 125.42523, 101.01407, 106.77289, 129.05862, 116.62911, 104.77397, 106.458466, 124.25252, 115.5475, 98.01916, 104.80032, 117.166016, 119.18422, 83.41682, 128.68199, 72.24267, 114.94594, 87.68051, 98.67698, 113.71978, 81.60204, 142.87119, 93.296585, 119.98569, 93.33039, 119.36511, 82.919785, 139.40448, 102.136314, 101.36266, 84.66713, 127.31614, 96.71986, 103.24164, 101.62049, 95.661415, 91.01312, 112.29405, 79.43339, 130.10623, 114.65978, 76.53104, 108.24987, 78.29704, 106.546196, 81.03192, 91.05619, 109.21216, 91.61465, 107.803795, 107.094925, 77.70952, 95.51498, 95.54441, 103.60028, 77.25259, 90.17917, 104.70095, 94.48449, 101.74158, 83.42015, 113.43296, 99.43712, 85.41394, 76.538666, 130.61087, 86.726204, 106.5455, 83.43552, 106.55038, 95.556, 85.79598, 97.04052, 80.45912, 92.19535, 95.53748, 99.77129, 88.56044, 86.38719, 106.43723, 99.562454, 73.08625, 77.76941, 116.25494, 75.712234, 117.75647, 95.13066, 79.61669, 94.84194, 81.55875, 81.527374, 107.538216, 96.26243, 76.393616, 82.29626, 99.299324, 104.63168, 70.78505, 80.07383, 108.70699, 74.77289, 118.134705, 78.97374, 113.518776, 96.42511, 80.6968, 90.22643, 83.236015, 72.803154, 115.49409, 95.19235, 86.51934, 75.42363, 112.067055, 86.36881, 89.894035, 83.14422, 94.95295, 88.03495, 84.094604, 84.42047, 89.02477, 94.107574, 73.94626, 79.99404, 102.508385, 72.67849, 111.50393, 92.79646, 76.210304, 89.11475, 83.90932, 87.38179, 85.9253, 89.63508, 80.63145, 69.46173, 120.76701, 86.81919, 86.23271, 82.791855, 82.30422, 67.391304, 120.28746, 93.45571, 66.338005, 88.61473, 75.70582, 89.9203, 74.04473, 78.065, 90.093605, 96.24112, 57.39769, 91.55211, 72.919266, 90.961266, 70.67494, 74.55639, 99.11819, 84.44482, 76.782715, 86.81444, 73.66124, 89.67382, 70.23203, 86.216995, 82.77884, 92.51851, 60.53182, 83.08468, 78.39639, 87.54936, 75.71346, 73.18228, 98.50977, 93.73708, 52.866062, 74.82197, 85.57423, 85.00695, 68.37227, 70.43183, 109.7832, 78.73998, 87.77047, 98.77582, 46.008396, 81.72994, 70.810036, 85.44308, 59.84027, 76.88672, 75.885574, 84.1264, 62.959976, 80.614426, 81.87493, 81.33137, 68.624985, 68.792564, 90.242165, 82.02825, 73.26478, 77.48976, 77.33503, 76.96815, 79.48034, 59.199123, 106.92814, 69.749245, 90.696884, 67.957794, 82.47514, 70.80462, 84.980896, 84.29319, 64.2352, 79.43791, 55.568665, 78.45161, 65.05486, 70.78938, 82.20697, 80.54031, 61.63297, 58.664364, 96.75216, 66.20805, 87.44949, 67.27102, 79.952225, 72.31614, 79.151505, 77.96256, 74.32757, 80.18242, 60.300766, 68.104195, 71.7725, 64.90104, 80.7154, 53.04452, 113.09765, 63.22187, 83.45258, 65.11753, 100.30983, 62.495495, 78.24539, 77.1511, 55.163258, 65.77531, 73.1433, 68.7658, 67.09275, 64.51115, 69.312454, 60.66525, 72.99361, 70.201454, 72.204994, 68.33777, 60.094837, 75.09128, 50.931355, 54.977135, 93.044785, 70.764084, 57.93088, 59.595634, 77.81166, 68.023964, 72.78125, 63.278843, 70.93757, 71.51118, 73.22927, 66.2238, 61.559647, 65.0193, 62.51806, 67.7001, 70.89525, 74.41837, 69.0066, 74.12277, 56.384663, 60.225212, 68.13305, 60.31738, 65.27119, 62.668797, 63.3115, 74.572624, 56.25602, 71.189865, 53.219723, 52.808475, 75.45058, 58.767063, 70.72589, 59.32254, 63.107746, 71.158745, 45.574913, 50.287952, 78.951805, 80.143814, 46.298466, 55.553406, 69.714714, 64.44988, 63.845554, 87.44074, 63.82351, 76.675674, 39.148834, 54.19972, 69.10224, 52.755444, 71.86355, 58.007374, 59.104187, 52.768093, 65.60211, 53.928825, 71.75518, 50.485157, 69.742294, 58.307014, 58.062332, 62.639835, 57.803936, 75.45787, 43.587097, 61.11072, 48.56553, 54.231396, 60.655006, 58.83721, 57.7833, 56.532047, 61.86832, 58.936543, 55.737034, 55.586857, 68.86933, 66.22368, 40.852688, 52.008835, 63.46099, 48.805485, 67.705246, 45.0128, 72.86516, 49.583263, 67.447876, 59.17922, 56.851433, 66.17177, 67.55345, 60.90485, 69.47742, 61.82816, 56.258156, 63.709877, 34.502384, 51.7503, 62.382057, 52.34122, 52.333214, 59.74133, 47.69918, 52.54086, 56.337532, 46.528736, 57.684925, 57.740932, 51.999825, 60.51843, 49.45612, 56.791847, 43.807682, 56.548218, 46.367645, 52.251045, 44.10973, 54.66035, 60.094204, 53.8762, 54.81234, 55.65623, 49.991573, 53.399323, 61.89559, 56.69913, 47.43744, 55.56462, 35.760075, 48.64434, 55.762894, 60.183968, 54.37902, 53.390305, 56.455856, 49.204033, 47.88161, 50.11095, 54.711437, 57.005882, 70.63575, 40.55387, 67.04745, 45.69239, 51.144653, 54.8221, 54.89211, 50.454144, 39.45451, 41.829575, 57.697807, 49.977657, 46.188503, 47.01752, 73.14368, 71.8592, 70.498375, 46.455315, 61.744106, 47.428486, 61.00421, 47.692642, 50.030792, 57.19303, 32.949482, 44.664906, 50.01788, 50.064148, 43.21997, 46.59433, 46.05089, 42.215786, 55.505547, 49.53981, 54.25306, 45.929127, 41.765125, 33.08903, 62.308727, 37.310585, 64.30982, 36.4173, 55.059895, 45.20802, 50.295326, 57.290638, 42.581596, 54.246265, 35.32832, 49.91761, 42.383404, 40.97255, 52.129223, 40.287785, 57.6717, 45.925888, 42.661716, 49.684444, 36.01718, 43.56374, 56.39067, 49.846176, 53.568367, 54.291046, 35.282516, 39.914555, 60.295063, 46.361473, 54.71822, 48.61713, 35.765182, 48.868256, 36.086514, 40.878407, 42.536697, 46.2448, 45.06457, 48.181686, 29.097172, 48.217846, 33.420986, 44.627666, 41.99823, 46.370605, 33.00194, 46.19666, 36.41794, 50.864136, 46.751, 39.305134, 53.48757, 49.92056, 36.775967, 45.51175, 50.517544, 46.37456, 50.31495, 48.910713, 45.42139, 45.46847, 40.43728, 46.96589, 31.9767, 38.157368, 38.794807, 35.97684, 50.8389, 40.27504, 40.27237, 46.903267, 30.093422, 36.42292, 50.4476, 43.398987, 40.550816, 40.889343, 35.244507, 47.283276, 28.802277, 42.328777, 39.7255, 45.558506, 36.258793, 33.22894, 43.52538, 40.087727, 38.476597, 36.534794, 47.681576, 39.40098, 34.170994, 35.152172, 48.527363, 40.65207, 38.221767, 29.62455, 49.891808, 40.972942, 36.90398, 29.965286, 50.3256, 49.11722, 36.277283, 38.56175, 33.47646, 40.533314, 33.26061, 36.839798, 39.06713, 36.505398, 38.163017, 45.446507, 40.532707, 38.43686, 31.924309, 39.373997, 31.817013, 28.603294, 46.022255, 34.719704, 47.802113, 53.379894, 35.551796, 40.614582, 46.995613, 44.24335, 22.964096, 41.20289, 27.84645, 36.777985, 35.22348, 37.792305, 28.86264, 36.168617, 34.741062, 37.070805, 34.555984, 36.58664, 33.588806, 42.213146, 25.689636, 38.13418, 40.84365, 34.275208, 38.38667, 41.099407, 32.023567, 39.70636, 28.110857, 34.305775, 41.330673, 27.02928, 51.764202, 40.388115, 29.25873, 32.841747, 36.79755, 32.71497, 38.591164, 35.26607, 27.377977, 33.10798, 32.589767, 33.26865, 35.01605, 28.954195, 46.38641, 32.615658, 42.839737, 33.82211, 34.08248, 34.03286, 25.139519, 31.075077, 30.25417, 43.5321, 29.35862, 34.86372, 34.027863, 29.912022, 32.679092, 29.724699, 36.588528, 28.23141, 42.3751, 35.33663, 34.729763, 30.83024, 41.401234, 30.056368, 36.841694, 29.926891, 38.523235, 31.693672, 34.986904, 30.002499, 34.696423, 32.29356, 34.068676, 32.76072, 29.413004, 33.24599, 28.881657, 26.531174, 41.11519, 29.591047, 39.259228, 32.12911, 29.91037, 30.483322, 38.89396, 55.136086, 34.77351, 32.284607, 28.720911, 24.812578, 36.68979, 26.09039, 38.452766, 24.622551, 38.247803, 29.175976, 32.89081, 29.53885, 29.562176, 26.957073, 34.784534, 26.557594, 36.541332, 28.344212, 30.062162, 33.914303, 20.750233, 28.0585, 31.666826, 31.721697, 31.530163, 25.947283, 26.978235, 27.683672, 29.313549, 25.77111, 34.943447, 26.074198, 29.198858, 36.140038, 19.122932, 60.16199, 36.71545, 31.08605, 20.619133, 30.92084, 29.354063, 31.376846, 21.159254, 32.380966, 27.429352, 32.91314, 28.732077, 29.281376, 24.782364, 24.205172, 32.250374, 28.641066, 26.067326, 28.950485, 33.122295, 32.509094, 25.838366, 29.139374, 19.601688, 27.780018, 19.876434, 22.903961, 35.2384, 37.297855, 24.849695, 25.711203, 33.405285, 50.58842, 29.90001, 28.770802, 36.717293, 24.716738, 35.427807, 24.201199, 35.40004, 31.263859, 23.486143, 21.67435, 33.018196, 25.65075, 31.563444, 32.797825, 20.14439, 27.158356, 24.767529, 23.989645, 24.891329, 27.950653, 22.919832, 30.711906, 35.59974, 26.896082, 31.19195, 28.215792, 18.167582, 24.960402, 28.152378, 20.906244, 33.968742, 31.56882, 20.646914, 21.190514, 29.437683, 19.651384, 35.98964, 34.58541, 28.076721, 25.41751, 32.696983, 25.723026, 24.155148, 24.924175, 26.659891, 27.273247, 22.777391, 23.859386, 28.284002, 22.000044, 26.16244, 18.99311, 31.681837, 27.35627, 36.355907, 47.133015, 31.225555, 27.077488, 27.73611, 24.019781, 22.584778, 21.477072, 22.106094, 25.892616, 24.19123, 21.6812, 33.39482, 22.392298, 21.106386, 20.80102, 25.38783, 24.685373, 19.387644, 22.259548, 27.630959, 28.188385, 22.382057, 30.679062, 23.410618, 23.141706, 28.13013, 19.779057, 20.480446, 20.145863, 21.744007, 26.483326, 21.85526, 23.35677, 15.856308, 28.515522, 24.49514, 21.985727, 27.671947, 26.684328, 15.065184, 20.56737, 33.07558, 28.723507, 22.42616, 17.482212, 39.1733, 27.21286, 22.197155, 22.612091, 27.220436, 21.364319, 20.816265, 19.846796, 28.758635, 22.806591, 22.04413, 24.704157, 21.825958, 21.87424, 24.386805, 20.899061, 21.651014, 30.045408, 22.087677, 21.09094, 23.64868, 23.630384, 27.054007, 23.475096, 20.033833, 23.280737, 16.838251, 20.498676, 20.696856, 19.921223, 19.449688, 20.352514, 20.76877, 22.766283, 17.40673, 20.182446, 23.591654, 21.253498, 25.170841, 19.908037, 21.256968, 19.547396, 22.655846, 18.832535, 21.338558, 21.798456, 21.516338, 30.188753, 45.09992, 34.64295, 29.598663, 21.619223, 28.412355, 22.845196, 18.05307, 24.078014, 15.492786, 18.82793, 22.991198, 23.078642, 17.740168]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9bac6ddd50>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(history.losses)),history.losses) \n",
    "plt.show()\n",
    "print(history.losses)\n",
    "plt.plot(range(len(history.val_losses)),history.val_losses) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = lstm.predict_sequences_multiple(model, X_test, seq_len, 50)\n",
    "predicted_train = lstm.predict_sequences_multiple(model, X_train, seq_len, 50)\n",
    "plot_results(predicted_train[:50], y_train[:50], False)\n",
    "plot_results(np.concatenate([predicted_train,predicted]), np.concatenate([y_train,y_test]), True)\n",
    "plot_results_multiple(predicted[:50], y_test[:50], 50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = lstm.predict_sequence_full(model, X_test, seq_len)\n",
    "predicted_train = lstm.predict_sequence_full(model, X_train, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
